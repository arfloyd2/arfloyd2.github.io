Variable Selection

  The process of variable selection is a vital step for regression modelling. Thus, for the sake of clarity, and gaining a wide perspective of your data, I think that the process of choosing the variables should pull from multiple methods instead of just one. I believe that the overall goal of variable selection is two tiered. The first tier is making sure that you have all of the variables that adequately help to explain the variation in your response, or the variation in the overall pattern of the data (depending on if you are doing supervised or unsupervised learning). The second tier is to have the optimal number of variables in your data – enough to explain patterns and variations, but not too many (so as to cause the regression model to overfit and artificially inflate your criteria values such as R^2 and deflate the error variance of the model).     
  
  To make sure that the first tier is accommodated, I would first want to see which explanatory variables have the strongest relationship with the response variable (if I am working on supervised learning), or which variables capture most of the patterns in the data (for unsupervised learning). I think that a good first step is to look at visuals such as scatter plots, classification trees, and color-coded correlation matrices. The scatter plots and the correlation matrices in particular can provide some insight into which variables have strong relationships (linear or possibly polynomial- for the scatter plots) with each other and the response. For fitting a linear regression model, for example, I would be interested in keeping the variables that have a strong correlation to the response. It must also be noted that just because an explanatory variable might not have a strong bivariate relationship with the response, that doesn’t mean that the relationship wouldn’t be significant if estimated with other variables, so I would want to keep that in the back of my mind to prevent from prematurely excluding variables. Also, looking at multiple types of visuals will allow me to see what type of transformation ( if any ) would be needed to the explanatory variable in order for it be useful in the model. Graphs like scatterplots and line graphs can show a strong polynomial, exponential, or logarithmic relationship between a response and an explanatory variable - not just a linear relationship which is what the correlation matrix would illustrate. Next, I would want to incorporate statistical tests such as individual significance, Type III tests, and other variable significance tests to possibly see how important the variables are to the entire model on their own and when accounting for other variables. Finally, I would want to see how much variation is explained within the model given the chosen variables, and how well does the model predict values for the response with the current variables that are in use. Thus, the use of fit statistics such as AIC, BIC, Adjusted R-squared, Mallows CP and other results from goodness of fit tests to see how well the model is fitting would be helpful.       
 
  Once I get a good handle on the effectiveness of the model given the variables that are used, I would be interested in the second tier of making sure that there are enough variables that explain the variation, but not too many that would over fit and artificially inflate the goodness of fit statistics. Efficiency and robustness of a regression model are just as important as the effectiveness of the model. This is where I would start looking at significance results, multicollinearity, and variations of stepwise methods for variable selection. I’ve read quite a few critiques of stepwise variable selection such as forward or backward (some saying that the criteria of solely observing a p-value to determine whether or not a variable should be included in the regression is extremely flawed). However, I do feel that for some who are starting out with gaining experience in regression modelling, stepwise selection could be a helpful introduction in seeing how the regression coefficients of variables can perform. Then, as you gain more experience in variable selection, and see the limitations of stepwise methods, you can find different variations of stepwise selection methods ( that are not subject to the limitations of the stepwise methods). With that, I would use backward and forward variable selection to see which variables would be dropped or excluded during both runs. Then, I would look at ways to exclude variables that would contribute to collinearity within the model , but that wouldn’t hurt the overall fit of the model. There are many other methods that can be used during the phase, such as thinking of the tradeoff between predictability and variance-bias that should be considered, but for the sake of brevity, I would say that keeping the end goal of the regression in mind is essential for variable selection. 
![image](https://github.com/arfloyd2/arfloyd2.github.io/assets/142931914/8ab11a60-0fb9-41b2-b968-fc0a7b1c8dc5)
